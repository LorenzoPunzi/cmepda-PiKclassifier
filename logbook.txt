Logbook for CMEPDA final assignment     Ruben Forti / Lorenzo Punzi

09/01/22:

Abbiamo creato il repository con il Logbook.

Possiamo approfondire la questione della continuous integration per il testing e sphynx per la documentazione.

Abbiamo discusso un po' la separation power per introdurla ed eventualmente discuterla nel lavoro

Abbiamo delineato una prima potenziale parte del progetto (aka BOLDf). Questa ha lo scopo di paragonare direttamente una DNN al BoldR usato in morelloproject. Si fa uso solo di Mpipi e si applica ai dati generati con la frazione nota già misurata in morelloproject.

BOLDf:
1) Usare i MC per generare un set di "dati" con una frazione nota, ad esempio 0.5 che è sia vicina a quella target ed è unbiased a priori. Nel generarli salvare quali dati sono K (y=1) e quali Pi (y=0) per il training
2) Trainare la DNN su questo set di dati
3) Applicare la DNN ai "dati originali" e SOMMARE le yi ottenute su tutti gli eventi. Questa somma viene presa come il numero MEDIO di K nel sample di dati. Questo approccio ha il vantaggio di non fregarsene di ogni evento individuale ma di fare un "valore atteso totale" del numero di K, contando ogni evento pesandolo con probabilità di essere K, quindi per convenzione K=1. Ciò dovrebbe essere anche corretto nel caso (realissimo) in cui le probabilità NON sono fra loro indipendenti, tanto il valore atteso della somma è sempre la somma dei valori attesi. Quindi invece di fare una DNN con uscita {False,True} o trasformare le yi reali in 0 o 1 con un qualche criterio, PRESERVIAMO l'informazione data da un valore intermedio pesandolo come tale nella somma (In un certo senso è come se ogni evento ti dicesse "se tutti gli eventi sono come me f = yi", e si facesse una media di questi valori pesando tutti gli eventi ugualmente).
4) Si paragona la frazione così ottenuta con quella di BoldR. Bisognerà un po' esplorare il problema dell'incertezza da legare a questa stima. Per esempio noi calcoliamo il valore medio, ma se davvero fossero per esempio tutte massimamente correlate le probabilità potrebbe in linea di principio dare un valore di f intermedio anche quando nella realtà è molto vicino a 0 o 1....

Si possono estendere queste tecniche anche con un training su più frazioni invece di solo 0.5, eventualmente trainando la DNN su frazioni diverse e minimizzando la loss tenendo conto anche di questo iperparametro

Quanto detto fin'ora in linea di principio è fattibile con il solo Python, eventualmente PyRoot, ed eventualmente anche il JITing di C++ all'interno.


-----------------------------------------------------------------

11/01/22:

Iniziato il modulo import_functions, contenente le funzioni che permettono di
arrivare ad ottenere un array da sottoporre al training.

Utilizzato uproot per trasformare i dati dei tree in array numpy: oltre ad essere
più flessibile (permette di dare in input anche il nome della variabile desiderata),
impiega 1/3 del tempo della funzione costruita "a mano".

Le altre due funzioni servono per valori casuali dalle distribuzioni, assegnare
la flag ad ogni evento e fare un merge degli array delle due specie.

Per queste funzioni vanno scritti unit tests e documentazione

Queste funzioni sono utilizzate in dataset.py


12/01/22:

Documentazione delle prime funzioni fatta. Impostato il primo unit test (vedere
se va bene o è migliorabile), secondo u.t. da fare

16/01/22:

C'è un problema concettuale: Noi facciamo quello che vogliamo ma alla fine anche se si trovasse perfettamente la frazione di pi e K ( i.e. separazione perfetta) COMUNQUE non è la vera f, ma solo una variabile binomiale che IN MEDIA fa f (questo è il significato della sigma_best nel potere di separazione). Questa cosa non era stata chiarita in morelloproject, ma è fondamentale ricordare che f misurata NON è f0 ma una variabile a sua volta.

Si può fare eventualmente un bootstrap per misurare la varianza del nostro stimatore neurale finale, sennò considerare la somma delle varianze dei contributi yi

TH1::GetRandom ha il vantaggio di non dover tirarsi fuori un template ma ha lo svantaggio di essere distribuito non come la distribuzione ma come la pdf*U[binwidth] e inoltre i contents di ogni bin non sono che una poissoniana che in media fa l'integrale della pdf in quel bin---> fluttuazioni per sample size finita. Questi effetti sistematici se si usa GetRandom vanno quantomeno menzionati

Bisogna capire che generatori random usare dato che TRandom si basa su Rndm che a quanto pare NON va usato in studi statistici

Chiameremo f0 (f PURE) la frazione media da cui provengono le distribuzioni, mentre quella effettiva (f EFFECTIVE) è quella effettiva di K su pi sul sample LIMITATO di eventi

Consideriamo oltre che alle classiche stragie di functional programming la possibilità di costruire delle classi ibride array-TH1 ad hoc per potersi giostrare velocemente fra le due specie

Per il disegno per esempio potremmo creare una UNICA funzionzione arrtohist che converte uno nell'altro capendo che tipo è l'argomento e restituendo l'altro tipo di conseguenza

17/01/23:

Creiamo il training.py inizialmente per array di sola UNA colonna di dati (+ 1 di flag). Poi lavoreremo a generalizzare ad array a più colonne, cioè quando si usano multiple varibles per il training e la evaluation.
Sorge subito un problema: il training restituisce lo stesso valore per tutte gli inputs del data sample, o quantomeno sbaglia di molto (anche trainando su 0.5 e applicandolo a 0.4) e dà yi tutte molto vicine fra loro.
Questo potrebbe essere dovuto al fatto che gli stiamo dando soltanto una variabile in input. dovremmo provare a testare con due colonne, cioè due variabili.

19/01/23:

20/01/23 (incontro Rizzi):

Innanzitutto la somma dei valori yi in uscita non è un buono stimatore (come ha dimostrato Rizzi). Perciò sarà necessario fare un template fit sulla distribuzione di y in uscita dei dati usando come template le distribuzioni y dei dati di training.

Cose da fare:
    - Rimettere a posto i file .py nella cartella data/
    - Template fit/sistema lineare a valle della DNN
    - Cornerplot per vedere la separazioni fra variabili
    - Moduli per template fit classico / sistema lineare classico
    - Perfezionare la DNN
    - Boost decision tree
    - Grafico RoC per varie strategie




21/01/23 (Ruben):

Creata cartella contenente gli scripts per il template fit classico. L'upload
dei dataset conviene farlo con RDataFrame che è agilissimo e evita di fare
esplicitamente loop e dichiarazioni inutili di variabili (boilerplate addiòs)

Il fit non è terminato e l'organizzazione delle funzioni è da migliorare, ad
ora la cartella contiene:
  - template_fit.py -> funzioni di base per i fit dei due dataset mc
    e poi per il fit congiunto dati i rispettivi output (parametri di fit che
    si passano comodamente tra funzioni)
  - plot_utilities.py -> modulo che dovrebbe raccogliere funzioni comode per
    fare i plot degli istogrammi con sopra i fit: andrà implementato anche un
    subplot per i residui e andrà migliorata la schermata con le informazioni
    del fit (parametri, chi2, prob)



30/01/23 (Ruben):

Template fit sui montecarlo sono ultimati (andrebbe rivisto quanto sono affidabili,
cioè se il fit è robusto rispetto a piccole variazioni nel range ecc). Il fit sui
dati è impostato ma va perfezionato.

Ciao da Elia

I comandi da eseguire per compilare la shared library, dati i file header.h e
sourcefile.cpp, sono:
    gcc -c -fpic fit_functions.cpp `root-config --cflags --glibs` -o fit_functions
    gcc -shared -fpic -o func_library.so fit_functions


31/01/23 (Lorenzo e Ruben):

Abbiamo trovato un piccolo errore nel template fit e siamo riusciti a trovare il valore "giusto" per f.

Lorenzo ha problemi a montare le .so, ma non a usarle se già create...

Le priorità ora sono capire corner.py e il template fit a valle della DNN (se questa si riesce a far funzionare bene).


03/02/23 (Lorenzo):

Ho creato il file cornerplot.py per fare i corner plot. Devo mandare in input
array a più variabili di soli pi o soli k dai mc, e per farlo bisognerà fare uno
script utilizzando la funzione loadvars in import_functions.py, che non ricordo
chi ha scritto (ps: è stato Ruben). Non mi sembra che venga usata in nessuno
degli script presenti (ps: era usata nel main di import_functions stessa),
quindi la provo a usare.

Inoltre non capisco bene la differenza fra data_gen e training_set, sembrano fare
più o meno la stessa cosa. Non ho capito chi è stato a generare i multivariable
array xxxx_array_prova.txt (ps: è stato import_functions nel main, vedi sopra).
Tra l'altro data_gen ha un serio problema, la funzione prende solo due argomenti
mentre gliene viene dati 3 (anche f0) nel main.

Infine dataset.py non capisco a cosa serva, se è obsoleto cancelliamolo. Lo stesso
dataset_generation.py, che però vedo che è più recente. (ps: A quanto pare possono
entrambe essere cancellate)


04/02/23 (Lorenzo):

Ho rinominato /template_fit/import_functions.py come /template_fit/template_functions.py
modificando anche l'import in /template_fit/template_fit.py, così da non confondersi
con /data/import_functions.py

Potrebbe essere opportuno rinominare la cartella /data --> /arraygen dato che è
forse più pertinente come nome

Per qualche motivo non funziona if __name__ == '__main__' in corner.py, devo fare diretto...

Ho fatto corner.py, in particolare due funzioni: una fa il corner plot dato un
array, l'altra date liste di array fanno un corner plot sovrapposto degli array.

Ho runnato il cornerplot sovrapposto per tutte le scelte possibili di variabili
e praticamente tutte apparte le M0_Mxx sembrano troppo simili fra i pi e i k: QUESTO È UN SERIO PROBLEMA.
Ho salvato le varie combinazioni come immagini per poterle vedere in seguito.


05/02/23 (Ruben):

I corner plot in effetti non sono molto promettenti, ma ci sono un paio di cose
che si possono tentare:
    - nelle correlazioni tra q.tà cinematiche delle singole particelle, il massimo
      di alcune distribuzioni (tutte escluso eta) è diverso per i due dataset.
      Ora, i due MC raccolgono un numero leggermente diverso di eventi (differenza
      di circa 1000), ma questo effetto nelle distribuzioni può essere dovuto a
      una effettiva differenza di comportamento nelle due specie?
    - possiamo provare a valutare nuove variabili correlando quelle esistenti:
      ad esempio si potrebbe fare "M_p + M_KK", "M_p - M_PiPi" ecc (tutto ciò
      si fa velocemente con RDataFrame). La composizione di queste nuove variabili
      si basa sui plot di correlazione prodotti con corner, l'obbiettivo è di creare
      delle "slices" in tali plot dove si renda maggiormente evidente la distinzione
      tra regione rossa e blu


09/02/23 (Ruben):

Piccoli aggiustamenti alle funzioni nella cartella /data:
    - il nome della cartella per ora lo lascerei così perché sarebbero da cambiare
      i path dei file ovunque. Questo non è un problema ma va tenuta comunque attenzione
      alla questione dei path per sviluppare (alla fine) un sistema flessibile per
      raggiungere i file con i dati (es. funzione os.path)
    - nuova funzione dummy "array_to_txt" in import_functions (per sgravare così il
      main da inutili righe di codice ----> da fare)
    - funzioni in import_functions classificate in base al loro utilizzo


Mixing delle variabili: funzione lineare "v1+alpha*v2", l'alpha "ottimale" è quello
tale per cui un test di KS sui due sample rende una statistica maggiore. Per evitare
problemi di scala il range delle variabili è trasformato nel range [0,1]

In generale, guardando i risultati della statistica di KS c'è speranza di riuscire
a utilizzare efficacemente tutte le principali variabili (quindi anche P, MPiK, ...).
Non è detto che questo basti a far convergere tutto.

Il modulo merge_variables.py contiene tutto ciò che serve, e adesso salve le immagini
con le nuove distribuzioni confrontate con quelle vecchie. VA PRESTATA ATTENZIONE
AL WARNING RESTITUITO DAL TERMINALE quando si fa girare il codice: ci deve essere
un modo corretto di deallocare la memoria delle figure (e quindi di chiuderle),
ma ora come ora non saprei come implementarla.

Il merging tra masse miste MpiK e MKpi sembra promettente, lo è un po' meno quello
tra le masse MKK, Mpipi e l'impulso: vedere se con nuove combinazioni torna meglio
o se proprio non c'è nulla da fare


14/02/23 (Lorenzo):

Ho trasformato training.py in un modulo importabile con funzione deepneuralnetwork(...) e ho testato che funzioni chiamandola dal main dello stesso file.


14/02/23 (Ruben):

Migliorato merge_variables e runnato sulle combinazioni più promettenti. Salvati
in file txt le statistiche di KS per la nuova variabile e per le variabili iniziali.


15/02/23 (Lorenzo e Ruben):

Ricevuto il generatore di B da Michael e modificato per runnare su B senza background con adesso anche RICH. Generati i MC nuovi con RICH.

Modificato trining.py con train_dnn() (che volendo plotta anche i template di pi e K) e eval_dnn() separate.


16/02/23 (Lorenzo e Ruben):

Modificato cornerplot.py con parsing della shell delle variabili e mask sugli eventi con RICH = 999 per motivi di diplaying del cornerplot.

Bisogna riflettere sul fatto che solo il K ha alcuni (e non pochi) eventi con RICH1 e/o RICH2 = 999. Questo in teoria permette di discriminare per quegli eventi perfettamente i K dai pi (e forse sono proprio gli eventi che la rete ora con RICH prevede essere y =1). Tuttavia non è ovvio che sia realistica come procedura, dato che irl comunque i RICH avrebbero una efficienza<1 e quindi sarebbe possibile avere eventi senza RICH attivato (cioè 999) anche per PiPi. Una possibile soluzione sarebbe quella di applicare delle efficienze ad hoc (binomiale quando si filla la variabile hx_thetaCx) sull'impronta delle vere efficienze riportate da lhcb.

Costruita la funzione array_generator in import_functions.py, che vuole essere
la funzione principale per generare gli array di training e/o testing. C'è
qualche problema nell'includere le variabili merged: probabilmente andrà
ripensata la struttura di "mergevar" e del modulo in generale, poiché non è
comodo importare dai txt e le variabili vanno correttamente mixate anche per il
dataset dei dati veri (cosa che al momento non viene fatta) --> Penso di potermici
mettere sabato sera (Ruben), tanto è una cosa noiosa e si può andare avanti
benissimo anche senza

17/02/23 (Lorenzo):

Ho costruito la funzione dnn() in training.py che fa training di una dnn e evaluation
su un data sample, restituendo anche la evaluation di Pi e K. Ho costruito poi in
nnoutputfit.py la funzione find_cut() che trova la y_cut necessaria per avere una
certa efficienza/specificità sul taglio.


18/02/23 (Lorenzo):

In nnoutputfit.py ho costruito la funzione roc() che disegna la curva di roc relativa
al taglio in y di cui sopra. Ho inoltre implementato la risoluzione del sistema
lineare con le efficienze per trovare la frazione di K dato il taglio y_cut.

Ho poi risolto un problema legato alla forma delle variabili di cut come [...] invece
di semplici float: dopo vari tentativi e peripezie ho capito che gli array che escono
da model.predict() sono (n,1), non semplici array 1D. Quindi in eval_dnn() ho aggiunto
un .flatten() per farli diventare regolari array 1D.

Bisogna capire come mai le distribuzioni in y dei Pi e K sono così tanto variabili
da una trained dnn a un'altra. Ci sono delle spikes a 0,1 e 0,4x e non è chiaro
se i RICH = 999 siano leciti da usare o too easy.

Ho cercato di "giocare" un po' con i parametri della dnn per avere una risposta
stabile ma non mi è ancora risucito


19-20/02/23 (Ruben):

Confrontando "loadvars()" e "merge_variables.py", ho notato che si utilizza
uproot in due modi differenti: in loadvars si chiama uproot N volte per costruire
gli array delle N variabili; in merge_variables invece si chiama una volta sola
e i tree così estratti vengono quindi passati alla funzione mergevar() --> testando
loadvars col metodo originale e col secondo metodo il tempo totale del processo
risulta essere, rispettivamente 0.229 e 0.428 secondi: loadvars() quindi resta così
e si modifica mergevar() in modo da uniformare gli input alle funzioni.

21/02/23 + 22/02/23 (Lorenzo):

Ho creato dei test linspace per il find_cut() di nnoutputfit.py, non senza alcuni problemi.
Innanzitutto bisognerebbe capire più rigorosamente come fissare il delta di assertAlmostEqual(), dato che per ora l'ho fissato solo quanto basta per far passare il test (lol).

Il problema principale invece sta nell'importare nnoutputfit, che a sua volta importa training. Questo importing ricorsivo da ERRORE perché quando chiami il test lo fai dalla cartella tests, in cui NON esiste un file training da importare, ma lui prova a importarlo nel momento in cui test importa nnoutputfit. Per ora l'unica maniera che ho trovato di risolvere la questione è di importare il training solo nel main di nnoutputfit ma va trovata chiaramente una maniera migliore.

Ho in seguito aggiustato il funzionamento del test stesso corregendo la funzione

22/02/23 (Ruben):

Iniziato a guardare il merge delle variabili cinematiche con i cerenkov: i risultati
dipendono molto del numero di "999" nei cerenkov, quindi va chiarito intanto quel
punto per poi proseguire definitivamente

In generale il criterio (empirico) con il quale si accetta una combinazione è
deltaKS > 0.015, dove la variazione è riferita al KS maggiore dei sample originari

Commento training.py: un po' macchinoso gestire tutte le opzioni sparse per il codice,
potrebbe essere utile fare una classe apposta e definire l'oggetto opt_dnn che nei
suoi metodi raccoglie tutte queste opzioni


23/02/23 (Lorenzo):

Ho risolto il problmea dell'import: devi scrivere from .training import xxx,
usando cioè il relative import. Così non fa storie quando importi nnoutputfit da
tests. Tuttavia una cosa strana è che sembra metterci chiaramente di più a
runnare il relativo test (vorrei vedere se ci mette anche di più a runnare
nnoutputfit stesso), nonstante poi dica che il test ci ha messo i soliti 0.00x
secondi.

Inoltre ho iniziato a scrivere un test, stavolta "grafico" per la roc() function.
Il problema sembra essere il salvare la figura che compare nella roc() function,
anche mettendo una cartella fig dedicata nella cartella tests. Ho risolto usando
os.path.xxx per usare gli absolute paths ogni volta invece dei relative paths.

Riguardando meglio la parte con RDataFrame che usiamo per il template fit,
potremmo per esempio usare ROOT.EnableImplicitMT() per un po' di multithreading?
- Ruben: in linea di principio sì (e dovrebbe funzionare), però se provo a
  mettere il comando ROOT.ROOT.EnableImplicitMT() all'inizio del codice, questo
  non gira :(


25/02/23 (Ruben):

Costruita la classe che gestisce le opzioni della DNN e aggiornato di conseguenza
i file nnoutputfit e training.py

Valutazioni sul funzionamento della DNN:
  - il batch_size ottimale sembra essere 128, con valori più alti o più bassi le
    loss non scendono
  - A volte capita che le loss (soprattutto la training) scendano e poi risalgano,
    questo si ha spesso con un numero molto alto di parametri (>2000). NEWS: una
    prova più recente con 9991 parametri liberi e 500 epoche mostra una lenta
    discesa della training loss dopo la epoca 200, nonostante le risalite precedenti
    (figura "epochs_strangecase.pdf")
  - Visto il punto precedente, aggiungere layers non sembra consigliato; merita
    invece giocare sul numero di neuroni in 3/4 layer: la combinazione "finale"
    sperimentata è 60-45-30-15 (epochnum=200)
  - Ricordiamoci comunque che le sizes degli array di training e di testing sono,
    rispettivamente 100000 e 15000: potrebbe esserci una discreta dipendenza dalla
    finitezza del sample
  - Caso "ottimale" sperimentato (presente nella repo): la frazione stimata di K
    è 0.446, con efficienza del 90%. Entrambe le loss soffrono di oscillazioni,
    ma mentre il trend della training è sempre discendente, la validation presenta
    alcuni spikes --> stabilire se e come questo può rappresentare un problema e
    pensare a alcuni metodi per risolverlo, nel caso


26/02/23 (Ruben):

Costruito il test per il template fit, dove si richiede che nel caso in cui i dati
siano composti solo da pi o solo da k la stima restituita sia pari a 0 o 1 rispettivamente
(entro la seconda cifra decimale). Forse è meglio non tagliare alla seconda cifra
ma permettere valori stimati entro un certo intervallo (es. 3 sigma)?

Ora come ora il test è nella cartella apposita ma NON MI FUNZIONA L'IMPORT dei moduli
da un'altra cartella. Per verificare che il test funziona ho dovuto metterlo nella
cartella template_fit, cambiargli nome e poi chiamarlo semplicemente con python

27/02/23 (Lorenzo e Ruben):

Abbiamo finito di scrivere la ROC con sklearn e la relativa AUD. Abbiamo inoltre dato i tre angoli cherenkov dell'altra particella in pasto alla rete, ottenendo una separazione ancora più netta. Abbiamo ricreato i txt, stavolta prendendo il massimo di eventi possibili dai root MC generati attualmente (generati la settimana del 13/02), cioè 140000 Pi e 140000 K.

Ci siamo accorti di un doppio errore che fino a che Ruben non cambiasse
array_generator() non si era notato: prima i data.txt avevano comunque una
colonna di flag tutta a 1, adesso array_generator taglia l'ultima colonna al
data.txt, che quindi hanno una colonna in meno del train.txt. Su eval_dnn()
Lorenzo aveva scritto la funzione dando per scontato che non ci fosse l'ultima
colonna per i data.txt (che invece fin'ora c'era), però aveva sbagliato a
chiamare la funzione dal main quando evaluava i dati, chiamandoli come i MC, in
cui si da per scontato che abbiano la colonna di flag. Questi due effetti quindi
si cancellavano. Ora che array_generator taglia l'ultima colonna dei data.txt e
però è stato corretto l'errore, le cose di nuovo funzionano e ma non perché due
cose sbagliate si cancellano a vicenda.


INCONTRO RIZZI:

- Va bene quello che abbiamo fatto fin'ora
- Come immaginavamo non è per forza un male se la rete produce cose diverse a ogni training, l'idea è che uno a un certo punto lo fissa il training e fa il resto
    DA FARE:
- Ha senso la nostra idea di usare un'efficienza dei RICH (sensata, chiedere a qualcuno LHCb) in generazione dei MC per non rendere troppo OP la rete.
- Per migliorare la stabilità della rete potremmo provare batch rinormalization, dropout o diminuire il learning rate X
- Potremmo fare un boosted decision tree e disegnarne la ROC
- Potremmo fare con la Mpipi o Mkk una cosa analoga a quanto fatto con la y della rete, un taglo con certa efficienza, e vederne la ROC X
- Potremmo fare il template fit (a training fissato) delle pdf delle y in uscita dalla rete
- Per valutare le sistematiche possiamo provare pdf diversi oppure....


27/02/23 (Lorenzo):

Ho fatto mcut.py che però va runnato in inverse_mode e con ordine di array scambiato perché il cut deve essere verso Mhh<mcut per selezionare i K. Nel farlo ho cambiato alcune cose

- loadvars adesso ha un opzione in più per NON appenedere la colonna di flag per quando non serve, come in questo caso. Ho dovuto perciò cambiare anche array_generator che chiamava loadvars di conseguenza, semplificandolo (non Bisogna più rimuovere esplicitamente la colonna finale ai data.txt)
- Invece dei relative imports, che rompevano nel chiamare lo script da shell, ho usato absolute imports, e per fare ciò ho dovuto mettere cmepda-PiKclassifier nel mio PYTHONPATH. Ho modificato quindi più file .py del progetto con absolute imports
- Per semplicità ho creato la funzione get_filepaths() in import functions così da rendere veloce l'acquisizione dei paths dei .root files e quindi il conseguente loading delle variabili
- Ho aggiunto __init__.py alla root directory per eventuali usi di package futuri


28/02/23 (Lorenzo):

Aggiornato mcut, aggiungendo la possibilità di runnare la roc (ho dato ora questo nome alle previous rocsklearn e cambiato in roc_hombrew la nostra) in inverse mode e chiamandola così da mcut.py. Questo inverse mode manda tutto in 1-xxx perché il taglio qui è verso il BASSO, non verso l'altro come al solito (e come fatto per la dnn stessa).

Ho aggiunto anche l'opzione di flatten degli array che vengono da loadvars quando viene loaded solo una variable, dato che sennò di default vengono restituiti come vettori colonna. Chiamando loadvars così da mcut.py il mcut sembra essere nel posto giusto (molto verso destra dato che il 95% deve stare sotto), invece di circa a metà distribuzione come faceva ieri sera.

Ho trasformato il main di mcut.py in una funzione var_cut() che fa un taglio simile in generale in funzione di una qualsiasi variabile. Ho cambiato la find_cut() function così che siano distinti specificity mode e inverted mode, il latter ora significa fare un taglio verso il BASSO. Dunque ora non c'è più bisogno di chiamare con array scambiati come sopra.

Per semplicità ho creato la funzione get_txtpaths() in import functions così da rendere veloce l'acquisizione dei paths dei .txt files per le reti (get_filepaths-->get_rootpaths)

Added dtc which creates a decisiontreeclassifier with sklearn and trains it with a certain part of the known mc and evaluates it on the data and the remaining part of the mc to calculate efficiency and misid. This tran/test separation is necessary because it seems to perfectly predict the mc dataset used to train the tree itself, so it's sensless to use that same dataset to measure eff and misid (gives 1.0 and 0.0 respectively). This gives an estimated fraction of 0.457, eff = 0.91 and misid = 0.09.


01/03/2023 (Ruben)

Varie prove sulle opzioni della dnn (fatte con un sample ridotto per semplicità):
  - Batch normalization: non chiarissima la documentazione, ci sono le opzioni di "axis" e "adapt" che vanno gestite bene, per non togliere informazione. Dobbiamo comunque tenere a mente che potrebbe non essere troppo conveniente modificare le variabili in input, dato che potremmo
perdere informazioni preziose (ad esempio sulle code)
  - Dropout: fatti vari tentativi con 4 valori diversi, la situazione non è migliorata molto; potrebbe comunque essere comodo tenere un valore piccolo di dropout per evitare overfitting nel caso di un numero maggiore di epoche su sample più grandi
  - Learning rate: opzione più interessante, riducendo il valore di default di un fattore 10 o 20 non si osservano più spike enormi nella validation loss (che comunque oscilla ma ci sta) e la training loss scende tranquilla. Per i tentativi con lr=1e-4 e lr=5e-4 è stato usato il dataset "esteso".

Queste tre opzioni sono state aggiunte alla classe "dnn_settings" in caso di necessità

Fatta un'analisi con 5 neuroni, 400 epoche e lr=5e-5 (eff=0.95):
	fraction = 0.4455, ycut=0.442, misid=0.10, AOC=0.98



01/03/2023 (Lorenzo e Ruben):

Abbiamo aggiunto requirements.txt con pip .... tenendo solo i pacchetti che ci sembra di aver usato, manca ROOT !!!

Abbiamo generato nuovi array di MC con efficienze dei RICH (cambiando toy_Bgen.C) a 0.95 per ora, e fortunatamente non sono scomparsi i picchi a 0 e 1 rispettivi in templ_eval della nn



02/03/2023 (Lorenzo e Ruben):

Cambiato training.py --> deepnn.py
Creato utilities che ha tutta una serie di funzioni prese in giro da script preesistenti,
che abbiamo eliminato se non più necessari
import_functions.py --> import_datasets.py

Abbiamo fatto un sacco di pulizia in generale oltre a rivedere e correggere il generate_datasets.py


02/03/2023 (Lorenzo):
generate_datasets.py-->gen_from_toy.py

Ho cambiato il dtc(), dandole molto più opzioni così che la funzione possa buildare
sia da path root dati sia da path txt dati. Di default prende train_array.txt dato
che è più veloce.

Ho aggiunto anche l'opzione di far stampare su un txt il tree generato dal classificatore,
perché la figura veniva impossibile da displayare, troppo grossa, pesante e con
troppi branches illeggibili.

Ho cambiato varie liste a giro (in particolare i rootpaths e quindi quello che resituisce
default_rootpaths) con delle tuples, che sono più space e più time efficient.
Bisognerebbe farlo in generale ove possibile credo !!!! (okie dokie)


03/03/2023 (Lorenzo):

Ho pensato che il problema di loadare roba con relative paths... non è un problema
after all, perché alla fine serve solo per chiamare poi funzioni come uproot.xx
o np.loadtxt, che GIA funzionano bene se gli da un path relativo alla cartella da
cui vuoi importare il module: ho provato questa cosa chiamando array_generator da
una python shell in root_files e dandogli a mano i path dei root_files (che qui
non hanno ../data/root_files/xxx) e sure enough funziona perfettamente!

Ho provato a variare il decision tree, creando anche un REGRESSOR oltre che un classifier,
provando a trattare la y come una FEATURE e non una flag (unsupervised invece che
supervised). Solo che così impara TROPPO bene e quindi mi sputa in uscita per il
"validation" sample che ci creiamo solo 0 o 1, non azzeccandoci sempre ma non è
quello il punto, il punto è che non sputa fuori uno spettro tra 0 e 1 per le due
specie come deepnn come avevo sperato. Quindi nisba. F


04/03/2023 (Ruben):

Iniziato a lavorare sul main creando il parser e settando un primo insieme di opzioni.
L'idea con cui ho strutturato il codice dopo il parser è fare un bel ciclo FOR
su tutti gli elementi della lista "type" (che raccoglie i tipi di analisi richiesta
dall'utente); all'interno del ciclo for diversi "if" selezionano il tipo di analisi
da fare e chiamano la funzione corrispondente. Nel caso type = ["all"], le analisi
vengono fatte in ordine tranquillamente.

Una volta "compilato" il parser bisogna mettere qualche controllino per evitare
che vengano portate avanti richieste incoerenti ()

Ho provato a fare il template fit con questo metodo, il programma gira e il fit
viene effettuato, ma mi si sono sballati i fit sui templates, quindi escono
p-values non proprio belli (risolto)

Se riesco a trovarmi un po' di tempo domani implemento anche gli altri metodi.




06/03/2023 (Lorenzo):

Ho creato dei test per utils (Find_cut e roc) e per import_datasets (loadvars e
array_generator)

!!!! In array_generator quando si vuole N eventi si prendono i PRIMI N (o N/2),
ma potrebbero esserci dei problemi se si volesse chiamare questa funzione molte
volte di eguito sullo stesso campione no? Così sarebbe sempre lo STESSO set di eventi
preso, seppur in ordine ogni volta diverso per lo scrambling a VALLE.


07-08/03/2023 (Lorenzo):

Lista di varie cose fatte in questi due giorni:

- Creato le prime Exceptions, in particolare per il dtc e il loadheader (che però non ho potuto testare !!!!)
- Spostato lo shuffling degli array all'interno di loadvars, così che non si prendano sempre gli stessi eventi runnando più volte array_generator
- creato degli stat_xxx per valutare la distribuzione della f stimata per dtc e dnn
- implementato una maniera di salvare e quindi poter poi caricare senza dover retrainare le deepnn come .json files, con pesi salvati come .h5


08/03/2023 (Ruben):

Risolti i problemi con i template fit. Ho ripristinato la vecchia struttura di funzioni
all'interno del modulo template_fit.py, in modo da poter gestire da __main__.py
ogni opzione di ogni fit. Tra l'altro questo metodo risulta anche meno time-consuming
dell'altro (ordine decimi di secondo, nulla di che)

Portato avanti il __main__.py, ho ordinato le opzioni da passare alle funzioni
per il template fit e per la dnn. Va un attimo cercata una quadra su quali variabili
è utile passare al parser, ma andando avanti si sistemerà da sé. Resta da impostare
la sezione del decision tree e del var_cut (FATTO). Una cosa bellina sarebbe scrivere tutti
i risultati su un file .txt, ci si può lavorare (FATTO)


09/03/2023 (Lorenzo):

Aggiunto default_figpath così che le funzioni importate che plottano salvino nella
/fig della cwd, non della directory in cui sono definite queste funzioni.

Messi gli attributes di dnn_settings direttamente come options nella chiamata __init__
di istanziamento della classe per pulizia e pythonicità

Ho aggiunto splitting options ai file di stat_ così che si possa decidere quando s
i chiama in quanti pezzi slittare

Inoltre ho integrato lo stat_ nelle funzioni stesse per dt_class e var_cut, ma non
per dnn perché il point di come ho costruito stat_dnn è proprio quello di non dover
stare a ritrainare ogni volta la rete ma invece importarla






09/03/2023 (Ruben)

Completata la struttura del main con l'introduzione dei subparser: sono uno strumento
utile per raggruppare le options, ma di contro non si può (a quanto pare) eseguire
comandi di due subparser sulla stessa linea di comando. Quindi di fatto uno deve
fare in sequenza "gen" e poi "analysis" (il subparser per i plot non credo abbia senso)

Modifiche alla funzione gen_from_toy:
  - aggiunte exceptions per evitare che la frazione data in input sia fuori dall'
    intervallo [0,1] e per evitare che si richiedano più eventi di quanti ne siano
    disponibili nei file sorgente
  - adesso se num_mc=0 e num_data=0 il riempimento dei dataset viene "deciso" automaticamente,
    i vincoli che si pongono per determinare la dimensione dei datasets sono spiegati
    con un inline comment nel codice
  - adesso il nome del tree nei file .root di output è LO STESSO di quelli in input,
    cioè dei toyMC. Di default è stato messo quindi 't_M0pipi'


10/03/2023 (Ruben)

Main completato, volendo si possono aggiungere le seguenti cose:
    - prendere i parametri di una dnn GIA' ALLENATA e farli solo valutare
    - printare il punto dato dal DTC nel plot delle roc (insieme alle altre rocs) (Fatto)

Questione CI: file config.yml posto nella cartella .circleci è ereditato da un caso
in cui aveva funzionato. I test passano il comando "python -m pytest", quindi credo
non vi siano problemi da quel punto di vista. Resta il setup di circleCI (che deve
fare Lorenzo che è l'owner della repo) e da vedere eventuali problemi di building
che possono sorgere (e sorgeranno)





----------------
| COSE DA FARE |
----------------

- docs (lorenzo+ruben)  DA COMPLETARE
- rivedere i !!!!       DA FARE
- Sistematiche (lorenzo+ruben)      DA FARE
- Errori statistici (lorenzo)       QUASI FATTO (Mi sembra che ci siamo)
- Aggiungere errori statistici per DNN nel main (ruben)     IN PROGRESS
- CI (ruben+lorenzo)    PROVARE A COLLEGARE LA REPO SULL'APPLICAZIONE CIRCLECI E VEDERE CHE ERRORI DA'
- SPHYNX (lorenzo)      ?
- Range per var_cut and exception for wrong inverse mode. FRAZIONE CORRETTA IN specificity



------------------------
| COSE FATTE (DA POCO) |
------------------------
- exceptions (lorenzo)  OK
- tests (lorenzo)       OK
- parser/main (ruben)   OK
- relative path for get_xxx functions (lorenzo) OK
- upgrade dtc (lorenzo) OK
- Stampare a txt le frazioni dei vari metodi    SÌ, TUTTO OK


















----------------- IDEA GENERALE PROGETTO ---------------------

--- I/O DATA CON ROOT (RDF ove possibile)
--- FIT ROOT CLASSICO
--- ML:
    - DNN
    - BDT
--- PLOT---> TH1,CORNER,ROC
--- UNIT TEST
--- DOC (SPHYNX)
*** CONFRONTO RISULTATI



------------------------------- QUESTIONS -----------------------------

* How should the function modules be organized? Should we use only one module with all the functions? Or group related ones together?

* Can we/ should we use multiprocessing in one point or another?

* Is it bad practice to give a same names to options in different functions that call other functions? e.g. func(...inverse_mode = inverse_mode...)

* Is the way plot_options = [] is handled in eval_dnn() (as of 27/02) bad practice?

* In dnn can we calculate pi_eval and K_eval with the same MC data used for the training of the dnn itself (we are doing so at 28/02)? In the decisiontreeclass we separate the two things...
