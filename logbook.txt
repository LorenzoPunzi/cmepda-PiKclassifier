Logbook for CMEPDA final assignment     Ruben Forti / Lorenzo Punzi

09/01/22:

Abbiamo creato il repository con il Logbook.

Possiamo approfondire la questione della continuous integration per il testing e sphynx per la documentazione.

Abbiamo discusso un po' la separation power per introdurla ed eventualmente discuterla nel lavoro

Abbiamo delineato una prima potenziale parte del progetto (aka BOLDf). Questa ha lo scopo di paragonare direttamente una DNN al BoldR usato in morelloproject. Si fa uso solo di Mpipi e si applica ai dati generati con la frazione nota già misurata in morelloproject.

BOLDf:
1) Usare i MC per generare un set di "dati" con una frazione nota, ad esempio 0.5 che è sia vicina a quella target ed è unbiased a priori. Nel generarli salvare quali dati sono K (y=1) e quali Pi (y=0) per il training
2) Trainare la DNN su questo set di dati
3) Applicare la DNN ai "dati originali" e SOMMARE le yi ottenute su tutti gli eventi. Questa somma viene presa come il numero MEDIO di K nel sample di dati. Questo approccio ha il vantaggio di non fregarsene di ogni evento individuale ma di fare un "valore atteso totale" del numero di K, contando ogni evento pesandolo con probabilità di essere K, quindi per convenzione K=1. Ciò dovrebbe essere anche corretto nel caso (realissimo) in cui le probabilità NON sono fra loro indipendenti, tanto il valore atteso della somma è sempre la somma dei valori attesi. Quindi invece di fare una DNN con uscita {False,True} o trasformare le yi reali in 0 o 1 con un qualche criterio, PRESERVIAMO l'informazione data da un valore intermedio pesandolo come tale nella somma (In un certo senso è come se ogni evento ti dicesse "se tutti gli eventi sono come me f = yi", e si facesse una media di questi valori pesando tutti gli eventi ugualmente).
4) Si paragona la frazione così ottenuta con quella di BoldR. Bisognerà un po' esplorare il problema dell'incertezza da legare a questa stima. Per esempio noi calcoliamo il valore medio, ma se davvero fossero per esempio tutte massimamente correlate le probabilità potrebbe in linea di principio dare un valore di f intermedio anche quando nella realtà è molto vicino a 0 o 1....

Si possono estendere queste tecniche anche con un training su più frazioni invece di solo 0.5, eventualmente trainando la DNN su frazioni diverse e minimizzando la loss tenendo conto anche di questo iperparametro

Quanto detto fin'ora in linea di principio è fattibile con il solo Python, eventualmente PyRoot, ed eventualmente anche il JITing di C++ all'interno.


-----------------------------------------------------------------

11/01/22:

Iniziato il modulo import_functions, contenente le funzioni che permettono di
arrivare ad ottenere un array da sottoporre al training.

Utilizzato uproot per trasformare i dati dei tree in array numpy: oltre ad essere
più flessibile (permette di dare in input anche il nome della variabile desiderata),
impiega 1/3 del tempo della funzione costruita "a mano".

Le altre due funzioni servono per valori casuali dalle distribuzioni, assegnare
la flag ad ogni evento e fare un merge degli array delle due specie.

Per queste funzioni vanno scritti unit tests e documentazione

Queste funzioni sono utilizzate in dataset.py


12/01/22:

Documentazione delle prime funzioni fatta. Impostato il primo unit test (vedere
se va bene o è migliorabile), secondo u.t. da fare

16/01/22:

C'è un problema concettuale: Noi facciamo quello che vogliamo ma alla fine anche se si trovasse perfettamente la frazione di pi e K ( i.e. separazione perfetta) COMUNQUE non è la vera f, ma solo una variabile binomiale che IN MEDIA fa f (questo è il significato della sigma_best nel potere di separazione). Questa cosa non era stata chiarita in morelloproject, ma è fondamentale ricordare che f misurata NON è f0 ma una variabile a sua volta.

Si può fare eventualmente un bootstrap per misurare la varianza del nostro stimatore neurale finale, sennò considerare la somma delle varianze dei contributi yi

TH1::GetRandom ha il vantaggio di non dover tirarsi fuori un template ma ha lo svantaggio di essere distribuito non come la distribuzione ma come la pdf*U[binwidth] e inoltre i contents di ogni bin non sono che una poissoniana che in media fa l'integrale della pdf in quel bin---> fluttuazioni per sample size finita. Questi effetti sistematici se si usa GetRandom vanno quantomeno menzionati

Bisogna capire che generatori random usare dato che TRandom si basa su Rndm che a quanto pare NON va usato in studi statistici

Chiameremo f0 (f PURE) la frazione media da cui provengono le distribuzioni, mentre quella effettiva (f EFFECTIVE) è quella effettiva di K su pi sul sample LIMITATO di eventi

Consideriamo oltre che alle classiche stragie di functional programming la possibilità di costruire delle classi ibride array-TH1 ad hoc per potersi giostrare velocemente fra le due specie

Per il disegno per esempio potremmo creare una UNICA funzionzione arrtohist che converte uno nell'altro capendo che tipo è l'argomento e restituendo l'altro tipo di conseguenza

17/01/23:

Creiamo il training.py inizialmente per array di sola UNA colonna di dati (+ 1 di flag). Poi lavoreremo a generalizzare ad array a più colonne, cioè quando si usano multiple varibles per il training e la evaluation.
Sorge subito un problema: il training restituisce lo stesso valore per tutte gli inputs del data sample, o quantomeno sbaglia di molto (anche trainando su 0.5 e applicandolo a 0.4) e dà yi tutte molto vicine fra loro.
Questo potrebbe essere dovuto al fatto che gli stiamo dando soltanto una variabile in input. dovremmo provare a testare con due colonne, cioè due variabili.

19/01/23:

20/01/23 (incontro Rizzi):

Innanzitutto la somma dei valori yi in uscita non è un buono stimatore (come ha dimostrato Rizzi). Perciò sarà necessario fare un template fit sulla distribuzione di y in uscita dei dati usando come template le distribuzioni y dei dati di training.

Cose da fare:
    - Rimettere a posto i file .py nella cartella data/
    - Template fit/sistema lineare a valle della DNN
    - Cornerplot per vedere la separazioni fra variabili
    - Moduli per template fit classico / sistema lineare classico
    - Perfezionare la DNN
    - Boost decision tree
    - Grafico RoC per varie strategie




21/01/23 (Ruben):

Creata cartella contenente gli scripts per il template fit classico. L'upload
dei dataset conviene farlo con RDataFrame che è agilissimo e evita di fare
esplicitamente loop e dichiarazioni inutili di variabili (boilerplate addiòs)

Il fit non è terminato e l'organizzazione delle funzioni è da migliorare, ad
ora la cartella contiene:
  - template_fit.py -> funzioni di base per i fit dei due dataset mc
    e poi per il fit congiunto dati i rispettivi output (parametri di fit che
    si passano comodamente tra funzioni)
  - plot_utilities.py -> modulo che dovrebbe raccogliere funzioni comode per
    fare i plot degli istogrammi con sopra i fit: andrà implementato anche un
    subplot per i residui e andrà migliorata la schermata con le informazioni
    del fit (parametri, chi2, prob)



30/01/23 (Ruben):

Template fit sui montecarlo sono ultimati (andrebbe rivisto quanto sono affidabili,
cioè se il fit è robusto rispetto a piccole variazioni nel range ecc). Il fit sui
dati è impostato ma va perfezionato.

Ciao da Elia

I comandi da eseguire per compilare la shared library, dati i file header.h e
sourcefile.cpp, sono:
    gcc -c -fpic fit_functions.cpp `root-config --cflags --glibs` -o fit_functions
    gcc -shared -fpic -o func_library.so fit_functions


31/01/23 (Lorenzo e Ruben):

Abbiamo trovato un piccolo errore nel template fit e siamo riusciti a trovare il valore "giusto" per f.

Lorenzo ha problemi a montare le .so, ma non a usarle se già create...

Le priorità ora sono capire corner.py e il template fit a valle della DNN (se questa si riesce a far funzionare bene).


03/02/23 (Lorenzo):

Ho creato il file cornerplot.py per fare i corner plot. Devo mandare in input
array a più variabili di soli pi o soli k dai mc, e per farlo bisognerà fare uno
script utilizzando la funzione loadvars in import_functions.py, che non ricordo
chi ha scritto (ps: è stato Ruben). Non mi sembra che venga usata in nessuno
degli script presenti (ps: era usata nel main di import_functions stessa),
quindi la provo a usare.

Inoltre non capisco bene la differenza fra data_gen e training_set, sembrano fare
più o meno la stessa cosa. Non ho capito chi è stato a generare i multivariable
array xxxx_array_prova.txt (ps: è stato import_functions nel main, vedi sopra).
Tra l'altro data_gen ha un serio problema, la funzione prende solo due argomenti
mentre gliene viene dati 3 (anche f0) nel main.

Infine dataset.py non capisco a cosa serva, se è obsoleto cancelliamolo. Lo stesso
dataset_generation.py, che però vedo che è più recente. (ps: A quanto pare possono
entrambe essere cancellate)


04/02/23 (Lorenzo):

Ho rinominato /template_fit/import_functions.py come /template_fit/template_functions.py
modificando anche l'import in /template_fit/template_fit.py, così da non confondersi
con /data/import_functions.py

Potrebbe essere opportuno rinominare la cartella /data --> /arraygen dato che è
forse più pertinente come nome

Per qualche motivo non funziona if __name__ == '__main__' in corner.py, devo fare diretto...

Ho fatto corner.py, in particolare due funzioni: una fa il corner plot dato un
array, l'altra date liste di array fanno un corner plot sovrapposto degli array.

Ho runnato il cornerplot sovrapposto per tutte le scelte possibili di variabili
e praticamente tutte apparte le M0_Mxx sembrano troppo simili fra i pi e i k: QUESTO È UN SERIO PROBLEMA.
Ho salvato le varie combinazioni come immagini per poterle vedere in seguito.


05/02/23 (Ruben):

I corner plot in effetti non sono molto promettenti, ma ci sono un paio di cose
che si possono tentare:
    - nelle correlazioni tra q.tà cinematiche delle singole particelle, il massimo
      di alcune distribuzioni (tutte escluso eta) è diverso per i due dataset.
      Ora, i due MC raccolgono un numero leggermente diverso di eventi (differenza
      di circa 1000), ma questo effetto nelle distribuzioni può essere dovuto a
      una effettiva differenza di comportamento nelle due specie?
    - possiamo provare a valutare nuove variabili correlando quelle esistenti:
      ad esempio si potrebbe fare "M_p + M_KK", "M_p - M_PiPi" ecc (tutto ciò
      si fa velocemente con RDataFrame). La composizione di queste nuove variabili
      si basa sui plot di correlazione prodotti con corner, l'obbiettivo è di creare
      delle "slices" in tali plot dove si renda maggiormente evidente la distinzione
      tra regione rossa e blu


09/02/23 (Ruben):

Piccoli aggiustamenti alle funzioni nella cartella /data:
    - il nome della cartella per ora lo lascerei così perché sarebbero da cambiare
      i path dei file ovunque. Questo non è un problema ma va tenuta comunque attenzione
      alla questione dei path per sviluppare (alla fine) un sistema flessibile per
      raggiungere i file con i dati (es. funzione os.path)
    - nuova funzione dummy "array_to_txt" in import_functions (per sgravare così il
      main da inutili righe di codice ----> da fare)
    - funzioni in import_functions classificate in base al loro utilizzo


Mixing delle variabili: funzione lineare "v1+alpha*v2", l'alpha "ottimale" è quello
tale per cui un test di KS sui due sample rende una statistica maggiore. Per evitare
problemi di scala il range delle variabili è trasformato nel range [0,1]

In generale, guardando i risultati della statistica di KS c'è speranza di riuscire
a utilizzare efficacemente tutte le principali variabili (quindi anche P, MPiK, ...).
Non è detto che questo basti a far convergere tutto.

Il modulo merge_variables.py contiene tutto ciò che serve, e adesso salve le immagini
con le nuove distribuzioni confrontate con quelle vecchie. VA PRESTATA ATTENZIONE
AL WARNING RESTITUITO DAL TERMINALE quando si fa girare il codice: ci deve essere
un modo corretto di deallocare la memoria delle figure (e quindi di chiuderle),
ma ora come ora non saprei come implementarla.

Il merging tra masse miste MpiK e MKpi sembra promettente, lo è un po' meno quello
tra le masse MKK, Mpipi e l'impulso: vedere se con nuove combinazioni torna meglio
o se proprio non c'è nulla da fare


14/02/23 (Lorenzo):

Ho trasformato training.py in un modulo importabile con funzione deepneuralnetwork(...) e ho testato che funzioni chiamandola dal main dello stesso file.


14/02/23 (Ruben):

Migliorato merge_variables e runnato sulle combinazioni più promettenti. Salvati
in file txt le statistiche di KS per la nuova variabile e per le variabili iniziali.


15/02/23 (Lorenzo e Ruben):

Ricevuto il generatore di B da Michael e modificato per runnare su B senza background con adesso anche RICH. Generati i MC nuovi con RICH.

Modificato trining.py con train_dnn() (che volendo plotta anche i template di pi e K) e eval_dnn() separate.


16/02/23 (Lorenzo e Ruben):

Modificato cornerplot.py con parsing della shell delle variabili e mask sugli eventi con RICH = 999 per motivi di diplaying del cornerplot.

Bisogna riflettere sul fatto che solo il K ha alcuni (e non pochi) eventi con RICH1 e/o RICH2 = 999. Questo in teoria permette di discriminare per quegli eventi perfettamente i K dai pi (e forse sono proprio gli eventi che la rete ora con RICH prevede essere y =1). Tuttavia non è ovvio che sia realistica come procedura, dato che irl comunque i RICH avrebbero una efficienza<1 e quindi sarebbe possibile avere eventi senza RICH attivato (cioè 999) anche per PiPi. Una possibile soluzione sarebbe quella di applicare delle efficienze ad hoc (binomiale quando si filla la variabile hx_thetaCx) sull'impronta delle vere efficienze riportate da lhcb.

Costruita la funzione array_generator in import_functions.py, che vuole essere
la funzione principale per generare gli array di training e/o testing. C'è
qualche problema nell'includere le variabili merged: probabilmente andrà
ripensata la struttura di "mergevar" e del modulo in generale, poiché non è
comodo importare dai txt e le variabili vanno correttamente mixate anche per il
dataset dei dati veri (cosa che al momento non viene fatta) --> Penso di potermici
mettere sabato sera (Ruben), tanto è una cosa noiosa e si può andare avanti
benissimo anche senza

17/02/23 (Lorenzo):

Ho costruito la funzione dnn() in training.py che fa training di una dnn e evaluation
su un data sample, restituendo anche la evaluation di Pi e K. Ho costruito poi in
nnoutputfit.py la funzione find_cut() che trova la y_cut necessaria per avere una
certa efficienza/specificità sul taglio.


18/02/23 (Lorenzo):

In nnoutputfit.py ho costruito la funzione roc() che disegna la curva di roc relativa
al taglio in y di cui sopra. Ho inoltre implementato la risoluzione del sistema
lineare con le efficienze per trovare la frazione di K dato il taglio y_cut.

Ho poi risolto un problema legato alla forma delle variabili di cut come [...] invece
di semplici float: dopo vari tentativi e peripezie ho capito che gli array che escono
da model.predict() sono (n,1), non semplici array 1D. Quindi in eval_dnn() ho aggiunto
un .flatten() per farli diventare regolari array 1D.

Bisogna capire come mai le distribuzioni in y dei Pi e K sono così tanto variabili
da una trained dnn a un'altra. Ci sono delle spikes a 0,1 e 0,4x e non è chiaro
se i RICH = 999 siano leciti da usare o too easy.

Ho cercato di "giocare" un po' con i parametri della dnn per avere una risposta
stabile ma non mi è ancora risucito


19-20/02/23 (Ruben):

Confrontando "loadvars()" e "merge_variables.py", ho notato che si utilizza
uproot in due modi differenti: in loadvars si chiama uproot N volte per costruire
gli array delle N variabili; in merge_variables invece si chiama una volta sola
e i tree così estratti vengono quindi passati alla funzione mergevar() --> testando
loadvars col metodo originale e col secondo metodo il tempo totale del processo
risulta essere, rispettivamente 0.229 e 0.428 secondi: loadvars() quindi resta così
e si modifica mergevar() in modo da uniformare gli input alle funzioni.

21/02/23 + 22/02/23 (Lorenzo):

Ho creato dei test linspace per il find_cut() di nnoutputfit.py, non senza alcuni problemi.
Innanzitutto bisognerebbe capire più rigorosamente come fissare il delta di assertAlmostEqual(), dato che per ora l'ho fissato solo quanto basta per far passare il test (lol).

Il problema principale invece sta nell'importare nnoutputfit, che a sua volta importa training. Questo importing ricorsivo da ERRORE perché quando chiami il test lo fai dalla cartella tests, in cui NON esiste un file training da importare, ma lui prova a importarlo nel momento in cui test importa nnoutputfit. Per ora l'unica maniera che ho trovato di risolvere la questione è di importare il training solo nel main di nnoutputfit ma va trovata chiaramente una maniera migliore.

Ho in seguito aggiustato il funzionamento del test stesso corregendo la funzione

22/02/23 (Ruben):

Iniziato a guardare il merge delle variabili cinematiche con i cerenkov: i risultati
dipendono molto del numero di "999" nei cerenkov, quindi va chiarito intanto quel
punto per poi proseguire definitivamente

In generale il criterio (empirico) con il quale si accetta una combinazione è
deltaKS > 0.015, dove la variazione è riferita al KS maggiore dei sample originari

Commento training.py: un po' macchinoso gestire tutte le opzioni sparse per il codice,
potrebbe essere utile fare una classe apposta e definire l'oggetto opt_dnn che nei
suoi metodi raccoglie tutte queste opzioni

23/02/23 (Lorenzo):

Ho risolto il problmea dell'import: devi scrivere from .training import xxx, usando cioè il relative import. Così non fa storie quando importi nnoutputfit da tests. Tuttavia una cosa strana è che sembra metterci chiaramente di più a runnare il relativo test (vorrei vedere se ci mette anche di più a runnare nnoutputfit stesso), nonstante poi dica che il test ci ha messo i soliti 0.00x secondi.

Inoltre ho iniziato a scrivere un test, stavolta "grafico" per la roc() function. Il problema sembra essere il salvare la figura che compare nella roc() function, acnhe mettendo una cartella fig dedicata nella cartella tests...























----------------- IDEA GENERALE PROGETTO ---------------------

--- I/O DATA CON ROOT (RDF ove possibile)
--- FIT ROOT CLASSICO
--- ML:
    - DNN
    - BDT
--- PLOT---> TH1,CORNER,ROC
--- UNIT TEST
--- DOC (SPHYNX)
*** CONFRONTO RISULTATI



------------------------------- QUESTIONS -----------------------------

* How should the function modules be organized? Should we use only one module with all the functions? Or group related ones together?

* Can we/ should we use multiprocessing in one point or another?
